{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-NN Classifier (Run the cell sequentially)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# load data .h5 to numpy array\n",
    "def load_data():\n",
    "    \n",
    "    #load training labels\n",
    "    with h5py.File('../Input/labels_training.h5','r') as H:\n",
    "        label=np.copy(H['label'])\n",
    "    trainLabel=label\n",
    "    \n",
    "    #load training data\n",
    "    with h5py.File('../Input/images_training.h5','r') as H:\n",
    "        data=np.copy(H['data'])\n",
    "    trainData=data\n",
    "    \n",
    "    #load test labels\n",
    "    with h5py.File('../Input/labels_testing_2000.h5','r') as H:\n",
    "        label=np.copy(H['label'])\n",
    "    testLabel=label\n",
    "    \n",
    "    #load test data\n",
    "    with h5py.File('../Input/images_testing.h5','r') as H:\n",
    "        data=np.copy(H['data'])\n",
    "    testData=data\n",
    "    \n",
    "    return trainLabel, trainData, testLabel, testData\n",
    "    \n",
    "\n",
    "# preprocess training & test data: flatten and reshaping    \n",
    "def preprocess_data(trainData, testData):\n",
    "    \n",
    "    # normalise training data from 0-255 to 0-1 (float)\n",
    "    trainData=trainData/255.0\n",
    "    \n",
    "    # normalise testing data from 0-255 to 0-1 (float)\n",
    "    testData=testData/255.0\n",
    "    \n",
    "    # reshape training data from 28*28 to 784\n",
    "    newTrain= trainData.flatten().reshape(30000, 784)\n",
    "    newTrain.shape\n",
    "    \n",
    "    # reshape test data from 28*28 to 784\n",
    "    newTest= testData.flatten().reshape(5000, 784)\n",
    "    newTest.shape\n",
    "\n",
    "    return newTrain, newTest  \n",
    "\n",
    "# create method to get distance from vector test to vector train\n",
    "def get_distances(newTest):\n",
    "\n",
    "    # get square value of test data\n",
    "    sqTest = np.square(newTest)\n",
    "    \n",
    "    # formed into (newTest,) matrix\n",
    "    totalTest=sqTest.sum(axis=1)\n",
    "        \n",
    "    # get square value of test data\n",
    "    sqTrain = np.square(newTrain)\n",
    "    \n",
    "    # formed into (30000,) matrix\n",
    "    totalTrain=sqTrain.sum(axis = 1)\n",
    "    \n",
    "    # get dot product of vectors\n",
    "    dotProduct = np.dot(newTest, newTrain.T)\n",
    "    \n",
    "    #calculate distance\n",
    "    distances = np.sqrt(-2 * dotProduct + totalTrain + np.matrix(totalTest).T)\n",
    "    \n",
    "    #distances=np.linalg.norm(newTest-newTrain)\n",
    "    return(distances)\n",
    "    \n",
    "def get_prediction(newTest, k):\n",
    "    \n",
    "    # call get_distance() and store the value to distances\n",
    "    distances=[]\n",
    "    distances = get_distances(newTest)\n",
    "    \n",
    "    #create new array with size equal to size of distances\n",
    "    totalTest = distances.shape[0]\n",
    "    \n",
    "    # create temporary array of labels with value 0 and size equal to totalTest \n",
    "    predictedClass = np.zeros(totalTest)\n",
    "    \n",
    "    for x in range(totalTest):\n",
    "        topK=[]\n",
    "    \n",
    "        # get the distances index which equivalen to label index, then store in labels as flat array\n",
    "        labels = trainLabel[np.argsort(distances[x,:])].flatten()\n",
    "\n",
    "        # get the top-K labels from labels data \n",
    "        topK = labels[:k]\n",
    "        c = Counter(topK)\n",
    "        predictedClass[x] = c.most_common(1)[0][0]\n",
    "\n",
    "    return(predictedClass)  \n",
    "\n",
    "def export_CSV():\n",
    "    \n",
    "    out_file = open(\"predictions_knn2000.csv\", \"w\")\n",
    "    out_file.write(\"ImageId,Label,Actual \\n\")\n",
    "    for i in range(len(predictions)):\n",
    "        out_file.write(str(i+1) + \",\" + str(int(predictions[i])) + \",\" + str(testLabel[i])+ \"\\n\")\n",
    "    out_file.close()\n",
    "    \n",
    "def export_CSV2():\n",
    "    \n",
    "    out_file = open(\"predictions_knn5000.csv\", \"w\")\n",
    "    out_file.write(\"ImageId,Label\\n\")\n",
    "    for i in range(len(predictions)):\n",
    "        out_file.write(str(i+1) + \",\" + str(int(predictions[i]))+ \"\\n\")\n",
    "    out_file.close()\n",
    "    \n",
    "def predict_2000(batchSize,k):\n",
    "    predictions = []\n",
    "    for x in range(int(len(newTest)/(2*batchSize))):\n",
    "        print(\"Starting prediction: \" + str((x+1)*batchSize) + \" of \" + str(int(len(newTest))))\n",
    "        start = time.time()\n",
    "        predictionResult = get_prediction(newTest[x * batchSize:(x+1) * batchSize], k)\n",
    "        end = time.time()\n",
    "        predictions = predictions + list(predictionResult)\n",
    "        print(\"Completed in \" + str(round((end-start),3)) + \" Secs.\")\n",
    "   \n",
    "    return (predictions)\n",
    "\n",
    "def predict_5000(batchSize, k, predictions):\n",
    "    for x in range(int(len(newTest)/(2*batchSize)), int(len(newTest)/batchSize)):\n",
    "        print(\"Starting prediction: \" + str((x+1)*batchSize) + \" of \" + str(int(len(newTest))))\n",
    "        start = time.time()\n",
    "        predictionResult = get_prediction(newTest[x * batchSize:(x+1) * batchSize], k)\n",
    "        end = time.time()\n",
    "        predictions = predictions + list(predictionResult)\n",
    "        print(\"Completed in \" + str(round((end-start),3)) + \" Secs.\")\n",
    "        \n",
    "    return (predictions)\n",
    "\n",
    "\n",
    "# construct confusion matrix for analysis\n",
    "\n",
    "def calConfusionMatrix(predictions):\n",
    "    # calculate the confusion matrix; labels is numpy array of classification labels\n",
    "    predictions=np.asarray(predictions)\n",
    "    predictions=predictions.astype(int)\n",
    "    classCount=len(np.unique(trainLabel))\n",
    "    cm = np.zeros(shape = (classCount, classCount))\n",
    "    \n",
    "    # loop through the prediction and actual result\n",
    "    for a, p in zip(testLabel, predictions):\n",
    "        #cm[a][p] += 1\n",
    "        cm[a,p] += 1\n",
    "    # return confusion matrix \n",
    "    return cm\n",
    "\n",
    "\n",
    "# get the analysis based on matrix\n",
    "def predictions_analysis(confMatrix):\n",
    "       \n",
    "    # copy a new matrix based on the supplied matrix\n",
    "    newMatrix = np.array(confMatrix)\n",
    "        \n",
    "    # true positives is the sum of diagonal values\n",
    "    TP=np.diag(newMatrix)\n",
    "    \n",
    "    # false negatives is the sum of row without TP\n",
    "    FN = np.sum(newMatrix, axis=1) - TP\n",
    "    \n",
    "    # false positive is the sum of column without TP\n",
    "    FP = np.sum(newMatrix, axis=0) - TP\n",
    "    \n",
    "    # true negative\n",
    "    num_classes = 10\n",
    "    TN = []\n",
    "    for x in range(num_classes):\n",
    "        temp = np.delete(newMatrix, x, 0)    # delete ith row\n",
    "        temp = np.delete(temp, x, 1)  # delete ith column\n",
    "        TN.append(sum(sum(temp)))\n",
    "        \n",
    "    # calculate precision, recall, accuracy, F1\n",
    "    precision = pd.DataFrame((TP/(TP+FP)),columns=['Precision'])\n",
    "    i=np.array(precision)\n",
    "    recall = pd.DataFrame((TP/(TP+FN)), columns=['Recall'])\n",
    "    j=np.array(recall)\n",
    "    accuracy = (np.sum(TP)/2000)*100\n",
    "    F1= pd.DataFrame((2*(i*j)/(i+j)),columns=['F1-Score'])\n",
    "\n",
    "    print (\"\\n Accuracy: \"+str(accuracy) +\"%\" +\"\\n\")\n",
    "    print (round(precision,3))\n",
    "    print()\n",
    "    print (round(recall,3))\n",
    "    print()\n",
    "    print (round(F1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: run load data method\n",
    "trainLabel, trainData, testLabel, testData=load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: run preprocess method\n",
    "newTrain,newTest=preprocess_data(trainData,testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 784) (5000, 784) (30000,)\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Sanity check for training, test, & label data\n",
    "print(newTrain.shape, newTest.shape, trainLabel.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: set batch size & number of K\n",
    "batchSize = 1000\n",
    "k=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting prediction: 1000 of 5000\n",
      "Completed in 3.216 Secs.\n",
      "Starting prediction: 2000 of 5000\n",
      "Completed in 3.111 Secs.\n",
      "[[148.   0.   3.   7.   1.   0.  18.   0.   1.   0.]\n",
      " [  1. 187.   0.   3.   0.   0.   0.   0.   0.   0.]\n",
      " [  4.   0. 169.   1.  18.   0.  18.   0.   0.   0.]\n",
      " [  5.   0.   6. 170.   7.   0.   3.   0.   0.   0.]\n",
      " [  0.   0.  24.   7. 158.   0.  22.   0.   1.   0.]\n",
      " [  0.   0.   0.   0.   0. 162.   1.  25.   1.  25.]\n",
      " [ 35.   0.  27.   6.  20.   0. 112.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   1.   0. 188.   0.   9.]\n",
      " [  1.   0.   2.   2.   2.   0.   1.   0. 210.   1.]\n",
      " [  0.   0.   0.   0.   0.   0.   1.   5.   0. 181.]]\n"
     ]
    }
   ],
   "source": [
    "# Step 5: run for 2000 test & analyze the performance\n",
    "predictions=predict_2000(batchSize,k)\n",
    "#export_CSV()\n",
    "confMatrix=calConfusionMatrix(predictions)\n",
    "print(confMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Accuracy: 84.25%\n",
      "\n",
      "   Precision\n",
      "0      0.763\n",
      "1      1.000\n",
      "2      0.732\n",
      "3      0.867\n",
      "4      0.767\n",
      "5      0.994\n",
      "6      0.636\n",
      "7      0.862\n",
      "8      0.986\n",
      "9      0.838\n",
      "\n",
      "   Recall\n",
      "0   0.831\n",
      "1   0.979\n",
      "2   0.805\n",
      "3   0.890\n",
      "4   0.745\n",
      "5   0.757\n",
      "6   0.560\n",
      "7   0.949\n",
      "8   0.959\n",
      "9   0.968\n",
      "\n",
      "   F1-Score\n",
      "0     0.796\n",
      "1     0.989\n",
      "2     0.766\n",
      "3     0.879\n",
      "4     0.756\n",
      "5     0.859\n",
      "6     0.596\n",
      "7     0.904\n",
      "8     0.972\n",
      "9     0.898\n"
     ]
    }
   ],
   "source": [
    "# Step 6: run the analysis based on the matrix\n",
    "predictions_analysis(confMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting prediction: 3000 of 5000\n",
      "Completed in 3.13 Secs.\n",
      "Starting prediction: 4000 of 5000\n",
      "Completed in 3.123 Secs.\n",
      "Starting prediction: 5000 of 5000\n",
      "Completed in 3.251 Secs.\n"
     ]
    }
   ],
   "source": [
    "# Step 7: run for the remaining 3000 test\n",
    "predictions=predict_5000(batchSize,k, predictions)\n",
    "# export CSV for 5000 test only (exclude label)\n",
    "#export_CSV2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Note : all classifier model has the same output name, please kindly take note\n",
    "# Step 8: save 5000 predicted label to .h5\n",
    "predictions=np.asarray(predictions)\n",
    "predictions=predictions.astype(int)\n",
    "with h5py.File('../Output/predicted_labels.h5','w') as H:\n",
    "    H.create_dataset('predictions', data=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000,) [3 4 4 1 0 2 4 5 7 6]\n"
     ]
    }
   ],
   "source": [
    "# Step 9: sanity check predicted_labels.h5\n",
    "with h5py.File('../Output/predicted_labels.h5','r') as H:\n",
    "    predictions=np.copy(H['predictions'])\n",
    "    sanityCheck=predictions.astype(int)\n",
    "print(sanityCheck.shape, sanityCheck[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
