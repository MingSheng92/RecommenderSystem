{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # autosave at most every 5 minutes\n",
    "%autosave 900"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "from keras.optimizers import Adam, Adadelta, Nadam, SGD\n",
    "from keras.layers import dot\n",
    "import keras.backend as K\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import gzip\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Data Preperation class :</b>\n",
    "<br/>\n",
    "The class will perform data pre-processing on the selected dataset and output a K-core trimmed dataset.<br/>\n",
    "<b>K_core_item : </b> only select items that has at least K ratings.<br/>\n",
    "<b>K_core_user : </b> only select users that has rated at least K items.<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_prep(object):\n",
    "    def __init__(self, fileName, K_core_item, K_core_user, conv=True):\n",
    "        self.fileName    = fileName\n",
    "        self.K_core_item = K_core_item\n",
    "        self.K_core_user = K_core_user\n",
    "        self.conv        = conv\n",
    "        \n",
    "    def filter_data(self):\n",
    "        data = pd.read_csv(self.fileName, header=None)\n",
    "        # rename the coumns in the dataset\n",
    "        if fileName == 'ratings_Electronics.csv' or fileName == 'data/ratings_Electronics.csv':\n",
    "            data = data.rename(columns={0: \"reviewerID\", 1: \"asin\", 2: \"overall\", 3:\"reviewTime\"})\n",
    "        \n",
    "        # cast to float32 to reduce memory \n",
    "        data['overall'] = data['overall'].astype('float32')\n",
    "        \n",
    "        # cut down item count by limiting the entry of the dataset, only include them when it has more than 20 ratings\n",
    "        item_rate = data['asin'].value_counts()\n",
    "        item_rate1 = pd.DataFrame(data=item_rate)\n",
    "        item_rate1.columns = ['rating_count']\n",
    "        rate_count = self.K_core_item \n",
    "        item_rate1.sort_values(by=['rating_count'])\n",
    "        item_rate1 = item_rate1.loc[item_rate1['rating_count'] >= rate_count]\n",
    "        item_list = item_rate1.index.values\n",
    "        data = data.loc[data['asin'].isin(item_list)]\n",
    "        \n",
    "        # cut down item count by limiting the entry of the dataset, only include them when it has more than 20 ratings\n",
    "        user_rate = data['reviewerID'].value_counts()\n",
    "        user_rate1 = pd.DataFrame(data=user_rate)\n",
    "        user_rate1.columns = ['rating_count']\n",
    "        rate_count = self.K_core_user\n",
    "        user_rate1.sort_values(by=['rating_count'])\n",
    "        user_rate1 = user_rate1.loc[user_rate1['rating_count'] >= rate_count]\n",
    "        user_list = user_rate1.index.values\n",
    "        data = data.loc[data['reviewerID'].isin(user_list)]\n",
    "        \n",
    "        # remove to free up RAM \n",
    "        del user_rate\n",
    "        del user_rate1\n",
    "        del user_list\n",
    "        del item_rate\n",
    "        del item_rate1\n",
    "        del item_list\n",
    "        \n",
    "        # if conversion of user and itemID is required\n",
    "        if self.conv:\n",
    "            # encode user and item ID\n",
    "            data['userID'] = data.reviewerID.astype('category').cat.codes.values\n",
    "            data['itemID'] = data.asin.astype('category').cat.codes.values\n",
    "        \n",
    "        return data\n",
    "    \n",
    "# return train, test and validation split\n",
    "def split(data, col_name, size, seed):\n",
    "    # split with statified to maintain distribution of the data\n",
    "    train, test = train_test_split(data,\n",
    "                                    stratify=data[col_name],\n",
    "                                    test_size=size,\n",
    "                                    random_state = seed)\n",
    "\n",
    "    # further split into train and validation set\n",
    "    train, val = train_test_split(train,\n",
    "                                    stratify=train[col_name],\n",
    "                                    test_size=size,\n",
    "                                    random_state = seed)\n",
    "    # train, test, validation\n",
    "    return train, test, val\n",
    "\n",
    "# parse function to unzip the gzip file\n",
    "def parse(path):\n",
    "    g = gzip.open(path, 'rb')\n",
    "    for l in g:\n",
    "        yield eval(l)\n",
    "\n",
    "# unzip the gzip file\n",
    "def getDF(path):\n",
    "    i = 0\n",
    "    df = {}\n",
    "    for d in parse(path):\n",
    "        df[i] = d\n",
    "        i += 1\n",
    "    return pd.DataFrame.from_dict(df, orient='index')\n",
    "\n",
    "#return the metafile propocessing \n",
    "def getMeta(file):\n",
    "    meta_data = getDF(file)\n",
    "\n",
    "    # drop any unnecessary columns for now\n",
    "    col = ['imUrl', 'categories','related', 'salesRank', 'brand', 'price'] \n",
    "    meta_data = meta_data.drop(col, axis=1)\n",
    "    \n",
    "    return meta_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_schedule(epoch):\n",
    "    lrate = 1.0\n",
    "    if epoch > 3:\n",
    "        lrate = 0.01\n",
    "    if epoch > 8:\n",
    "        lrate = 0.001\n",
    "    if epoch > 15:\n",
    "        lrate = 0.0001\n",
    "    return lrate\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    y_pred = K.clip(y_pred, 1, 5)\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true))) \n",
    "\n",
    "class Deep_SparseAutoEncoder(object):\n",
    "    def __init__(self, num_user, num_item, latent_factor, act, rr, pre_feed='concat', layer=[512,256]):\n",
    "        self.n_users    = num_user\n",
    "        self.n_items    = num_item\n",
    "        self.l_factor   = latent_factor\n",
    "        self.activation = act\n",
    "        self.reg_rate   = rr\n",
    "        self.pre_feed   = pre_feed\n",
    "        self.layer      = layer\n",
    "        self.model      = None\n",
    "            \n",
    "    def create_model(self, optimiser = 'Adadelta'):\n",
    "        # input \n",
    "        user_input = keras.layers.Input(shape=[1],name='User')\n",
    "        item_input = keras.layers.Input(shape=[1],name='Item')\n",
    "\n",
    "        # decompose users with embedding to get first level latent factor\n",
    "        user_embedding = keras.layers.Embedding(self.n_users + 1, self.l_factor ,name='User-Embedding')(user_input)\n",
    "        user_vec = keras.layers.Flatten(name='FlattenUsers')(user_embedding)\n",
    "        user_vec = keras.layers.Dropout(0.2)(user_vec)\n",
    "\n",
    "        # decompose items with embedding to get first level latent factor\n",
    "        item_embedding = keras.layers.Embedding(self.n_items + 1, self.l_factor, name='item-Embedding')(item_input)\n",
    "        item_vec = keras.layers.Flatten(name='FlattenItems')(item_embedding)\n",
    "        item_vec = keras.layers.Dropout(0.2)(item_vec)\n",
    "\n",
    "        # conbine the two embedding layer and feed into auto encoders as features\n",
    "        if self.pre_feed == 'concat':\n",
    "            x_inp = keras.layers.Concatenate()([user_vec, item_vec])\n",
    "        else:\n",
    "            x_inp = dot([user_vec, item_vec], axes=1, normalize=False, name='DotProduct') \n",
    "\n",
    "        # create layer based on the provided provided detail\n",
    "        for i in range(len(self.layer)):\n",
    "            # Encoder\n",
    "            # -----------------------------\n",
    "            layer_name = 'EncLayer'+ str(i+1) \n",
    "            x_inp = Dense(self.layer[i], activation=self.activation, name=layer_name,\n",
    "                        activity_regularizer=regularizers.l2(self.reg_rate))(x_inp)\n",
    "        \n",
    "        # bottleneck\n",
    "        x_inp = Dropout(0.5, name='Dropout')(x_inp) # Dropout\n",
    "\n",
    "        # create layer based on the provided provided detail\n",
    "        for i in reversed(range(len(self.layer))):\n",
    "            layer_name = 'DecLayer'+ str(len(self.layer) - i) \n",
    "            # Decoder\n",
    "            # -----------------------------\n",
    "            x_inp = Dense(self.layer[i], activation=self.activation, name=layer_name,\n",
    "                        activity_regularizer=regularizers.l2(self.reg_rate))(x_inp)\n",
    "\n",
    "        # Output \n",
    "        output_layer = keras.layers.Dense(1, activation='relu', name='Activation')(x_inp)\n",
    "        Adp_LR_method = Adadelta(lr=1.0, rho=0.95, epsilon=1e-08, decay=1e-6)\n",
    "\n",
    "        # define the final model\n",
    "        self.model = keras.Model([user_input, item_input], output_layer)       \n",
    "    \n",
    "        # compile the model and return \n",
    "        self.model.compile(optimizer= Adp_LR_method, loss='mean_squared_error', metrics=[root_mean_squared_error])\n",
    "        \n",
    "def evaluateModel(model, test_data):\n",
    "    y_hat = np.round(model.predict([test_data.userID, test_data.itemID]),0)\n",
    "    y_true = test_data.overall\n",
    "\n",
    "    y_hat = np.clip(y_hat,1 ,5)\n",
    "    MAE = mean_absolute_error(y_true, y_hat)\n",
    "    RMSE = np.sqrt(mean_squared_error(y_true, y_hat))\n",
    "\n",
    "    return MAE, RMSE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get data prepared with K-core Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileName = 'data/ratings_Electronics.csv'\n",
    "K_core_item = 20\n",
    "K_core_user = 20\n",
    "\n",
    "DP = data_prep(fileName, K_core_item, K_core_user, True)\n",
    "data = DP.filter_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>userID</th>\n",
       "      <th>itemID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>A1H8PY3QHMQQA0</td>\n",
       "      <td>0528881469</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1290556800</td>\n",
       "      <td>1065</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>AT09WGFUM934H</td>\n",
       "      <td>0594481813</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1377907200</td>\n",
       "      <td>7964</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>A2IDCSC6NVONIZ</td>\n",
       "      <td>0972683275</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1367280000</td>\n",
       "      <td>3318</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>A3BMUBUC1N77U8</td>\n",
       "      <td>0972683275</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1385164800</td>\n",
       "      <td>5182</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>AQBLWW13U66XD</td>\n",
       "      <td>0972683275</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1375574400</td>\n",
       "      <td>7800</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         reviewerID        asin  overall  reviewTime  userID  itemID\n",
       "17   A1H8PY3QHMQQA0  0528881469      2.0  1290556800    1065       0\n",
       "118   AT09WGFUM934H  0594481813      3.0  1377907200    7964       1\n",
       "189  A2IDCSC6NVONIZ  0972683275      5.0  1367280000    3318       2\n",
       "200  A3BMUBUC1N77U8  0972683275      4.0  1385164800    5182       2\n",
       "274   AQBLWW13U66XD  0972683275      5.0  1375574400    7800       2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, val = split(data, 'reviewerID', 0.2, 225)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we are ready to train Deep Sparse Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "n_users, n_items = len(data.userID.unique()), len(data.itemID.unique())\n",
    "latent_factor = 40\n",
    "activation = 'linear'\n",
    "reg_rate = 0.001\n",
    "pre_feed = 'concat'\n",
    "layers = [128, 64, 32, 16]\n",
    "\n",
    "DSAE = Deep_SparseAutoEncoder(n_users, n_items, latent_factor, activation, reg_rate, pre_feed, layers)\n",
    "DSAE.create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "User (InputLayer)               (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Item (InputLayer)               (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "User-Embedding (Embedding)      (None, 1, 40)        334320      User[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "item-Embedding (Embedding)      (None, 1, 40)        1710880     Item[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "FlattenUsers (Flatten)          (None, 40)           0           User-Embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "FlattenItems (Flatten)          (None, 40)           0           item-Embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 40)           0           FlattenUsers[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 40)           0           FlattenItems[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 80)           0           dropout_1[0][0]                  \n",
      "                                                                 dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "EncLayer1 (Dense)               (None, 128)          10368       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "EncLayer2 (Dense)               (None, 64)           8256        EncLayer1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "EncLayer3 (Dense)               (None, 32)           2080        EncLayer2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "EncLayer4 (Dense)               (None, 16)           528         EncLayer3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Dropout (Dropout)               (None, 16)           0           EncLayer4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "DecLayer1 (Dense)               (None, 16)           272         Dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "DecLayer2 (Dense)               (None, 32)           544         DecLayer1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "DecLayer3 (Dense)               (None, 64)           2112        DecLayer2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "DecLayer4 (Dense)               (None, 128)          8320        DecLayer3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Activation (Dense)              (None, 1)            129         DecLayer4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 2,077,809\n",
      "Trainable params: 2,077,809\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "DSAE.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 178540 samples, validate on 44636 samples\n",
      "Epoch 1/20\n",
      "178540/178540 [==============================] - 149s 832us/step - loss: 1.3658 - root_mean_squared_error: 1.0952 - val_loss: 1.2231 - val_root_mean_squared_error: 1.0594\n",
      "Epoch 2/20\n",
      "178540/178540 [==============================] - 148s 831us/step - loss: 1.1916 - root_mean_squared_error: 1.0543 - val_loss: 1.1821 - val_root_mean_squared_error: 1.06291.0\n",
      "Epoch 3/20\n",
      "178540/178540 [==============================] - 151s 845us/step - loss: 1.1246 - root_mean_squared_error: 1.0278 - val_loss: 1.0975 - val_root_mean_squared_error: 1.0182\n",
      "Epoch 4/20\n",
      "178540/178540 [==============================] - 149s 832us/step - loss: 1.0818 - root_mean_squared_error: 1.0093 - val_loss: 1.0942 - val_root_mean_squared_error: 1.0218\n",
      "Epoch 5/20\n",
      "178540/178540 [==============================] - 148s 827us/step - loss: 1.0444 - root_mean_squared_error: 0.9911 - val_loss: 1.0764 - val_root_mean_squared_error: 1.0096\n",
      "Epoch 6/20\n",
      "178540/178540 [==============================] - 147s 825us/step - loss: 1.0423 - root_mean_squared_error: 0.9903 - val_loss: 1.0765 - val_root_mean_squared_error: 1.0095\n",
      "Epoch 7/20\n",
      "178540/178540 [==============================] - 153s 858us/step - loss: 1.0403 - root_mean_squared_error: 0.9893 - val_loss: 1.0763 - val_root_mean_squared_error: 1.0099\n",
      "Epoch 8/20\n",
      "178540/178540 [==============================] - 165s 921us/step - loss: 1.0403 - root_mean_squared_error: 0.9888 - val_loss: 1.0758 - val_root_mean_squared_error: 1.0095\n",
      "Epoch 10/20\n",
      "178540/178540 [==============================] - 152s 852us/step - loss: 1.0391 - root_mean_squared_error: 0.9887 - val_loss: 1.0758 - val_root_mean_squared_error: 1.0096\n",
      "Epoch 11/20\n",
      "178540/178540 [==============================] - 148s 831us/step - loss: 1.0384 - root_mean_squared_error: 0.9877 - val_loss: 1.0758 - val_root_mean_squared_error: 1.0096\n",
      "Epoch 12/20\n",
      "178540/178540 [==============================] - 152s 849us/step - loss: 1.0385 - root_mean_squared_error: 0.9882 - val_loss: 1.0759 - val_root_mean_squared_error: 1.0097\n",
      "Epoch 13/20\n",
      "178540/178540 [==============================] - 145s 811us/step - loss: 1.0394 - root_mean_squared_error: 0.9883 - val_loss: 1.0758 - val_root_mean_squared_error: 1.0096\n",
      "Epoch 14/20\n",
      "178540/178540 [==============================] - 148s 827us/step - loss: 1.0395 - root_mean_squared_error: 0.9885 - val_loss: 1.0758 - val_root_mean_squared_error: 1.0096\n",
      "Epoch 15/20\n",
      "178540/178540 [==============================] - 146s 817us/step - loss: 1.0410 - root_mean_squared_error: 0.9892 - val_loss: 1.0757 - val_root_mean_squared_error: 1.0095\n",
      "Epoch 16/20\n",
      "178540/178540 [==============================] - 150s 843us/step - loss: 1.0394 - root_mean_squared_error: 0.9883 - val_loss: 1.0756 - val_root_mean_squared_error: 1.0096\n"
     ]
    }
   ],
   "source": [
    "#history = DSAE.model.fit([train.userID, train.itemID], train.overall, epochs=50, validation_split=0.2, verbose=1)\n",
    "history = DSAE.model.fit([train.userID, train.itemID], \n",
    "                    train.overall, \n",
    "                    epochs=20,\n",
    "                    validation_data=[[val.userID, val.itemID], val.overall],\n",
    "                    verbose=1, callbacks=[LearningRateScheduler(lr_schedule)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAE, RMSE =  evaluateModel(DSAE.model, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7358318\n",
      "1.076966\n"
     ]
    }
   ],
   "source": [
    "print(MAE)\n",
    "print(RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRecList(model, ratings, meta, user, top = 50):\n",
    "    # get the converted itemID\n",
    "    meta = pd.merge(meta, ratings[['asin','itemID']], on='asin', how='inner')\n",
    "    # remove any duplicates during merge\n",
    "    meta = meta.drop_duplicates(subset ='itemID', keep = 'first') \n",
    "    # not mandatory but just for better visualization \n",
    "    meta = meta.sort_values(by=['itemID'])\n",
    "    \n",
    "    # get all the avaialable item for rating\n",
    "    items = meta.itemID.reset_index().drop('index', axis=1)\n",
    "    # preparing dataset for prediction \n",
    "    items['userID'] = [user] * len(items)\n",
    "    \n",
    "    # get prediction of the all the available items\n",
    "    rec = np.round(model.predict([items.userID, items.itemID]),0)\n",
    "    # clip the prediction back to 1 - 5\n",
    "    rec = np.clip(rec,1 ,5)\n",
    "    \n",
    "    # add prediction score\n",
    "    items['pred'] = rec\n",
    "    # add product ID to the list\n",
    "    items['asin'] = meta_data.asin\n",
    "    # add title into the list\n",
    "    items['title'] = meta_data.title\n",
    "    # remove any unwanted na rows\n",
    "    items = items.dropna(subset=['asin'])\n",
    "    \n",
    "    Top_n = items.sort_values(by='pred', ascending=False).head(top)\n",
    "    \n",
    "    return Top_n\n",
    "\n",
    "def getPurcHist(userID, data, meta_data):\n",
    "    # get all the ratings in the ratings dataset\n",
    "    history_purchase = data.loc[data.userID == userID]\n",
    "    \n",
    "    # left join the meta dataset and history purchase \n",
    "    history_purchase = pd.merge(history_purchase, meta_data[['asin','title']], on='asin', how='left')\n",
    "    # drop review time \n",
    "    history_purchase = history_purchase.drop('reviewTime', axis=1)\n",
    "    \n",
    "    return history_purchase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#meta_data = getDF('meta_Electronics.json.gz')\n",
    "#col = ['imUrl', 'categories','related', 'salesRank', 'brand', 'price'] \n",
    "#meta_data = meta_data.drop(col, axis=1)\n",
    "\n",
    "meta_data = getMeta('meta_Electronics.json.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will try to get a reccomendation for a user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = 15518\n",
    "top = 20\n",
    "\n",
    "pHist = getPurcHist(user, data, meta_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recList recList = getRecList(DSAE.model, data, meta_data, user, top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pHist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.4\n"
     ]
    }
   ],
   "source": [
    "import keras; \n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.4\n"
     ]
    }
   ],
   "source": [
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow;\n",
    "print(tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21.2\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "print(sklearn.__version__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
